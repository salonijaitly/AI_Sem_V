{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nim.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPaKps8HzAqwfuT+amAFEo2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84mn-jgrKMH1"
      },
      "source": [
        "#Write an AI to predict whether online shopping customers will complete a purchase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xFDjCGzKHlk"
      },
      "source": [
        "#Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9ZObRcIJHVr"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import time\n",
        "\n",
        "\n",
        "class Nim():\n",
        "\n",
        "    def __init__(self, initial=[1, 3, 5, 7]):\n",
        "        \"\"\"\n",
        "        Initialize game board.\n",
        "        Each game board has\n",
        "            - `piles`: a list of how many elements remain in each pile\n",
        "            - `player`: 0 or 1 to indicate which player's turn\n",
        "            - `winner`: None, 0, or 1 to indicate who the winner is\n",
        "        \"\"\"\n",
        "        self.piles = initial.copy()\n",
        "        self.player = 0\n",
        "        self.winner = None\n",
        "\n",
        "    @classmethod\n",
        "    def available_actions(cls, piles):\n",
        "        \"\"\"\n",
        "        Nim.available_actions(piles) takes a `piles` list as input\n",
        "        and returns all of the available actions `(i, j)` in that state.\n",
        "        Action `(i, j)` represents the action of removing `j` items\n",
        "        from pile `i` (where piles are 0-indexed).\n",
        "        \"\"\"\n",
        "        actions = set()\n",
        "        for i, pile in enumerate(piles):\n",
        "            for j in range(1, piles[i] + 1):\n",
        "                actions.add((i, j))\n",
        "        return actions\n",
        "\n",
        "    @classmethod\n",
        "    def other_player(cls, player):\n",
        "        \"\"\"\n",
        "        Nim.other_player(player) returns the player that is not\n",
        "        `player`. Assumes `player` is either 0 or 1.\n",
        "        \"\"\"\n",
        "        return 0 if player == 1 else 1\n",
        "\n",
        "    def switch_player(self):\n",
        "        \"\"\"\n",
        "        Switch the current player to the other player.\n",
        "        \"\"\"\n",
        "        self.player = Nim.other_player(self.player)\n",
        "\n",
        "    def move(self, action):\n",
        "        \"\"\"\n",
        "        Make the move `action` for the current player.\n",
        "        `action` must be a tuple `(i, j)`.\n",
        "        \"\"\"\n",
        "        pile, count = action\n",
        "\n",
        "        # Check for errors\n",
        "        if self.winner is not None:\n",
        "            raise Exception(\"Game already won\")\n",
        "        elif pile < 0 or pile >= len(self.piles):\n",
        "            raise Exception(\"Invalid pile\")\n",
        "        elif count < 1 or count > self.piles[pile]:\n",
        "            raise Exception(\"Invalid number of objects\")\n",
        "\n",
        "        # Update pile\n",
        "        self.piles[pile] -= count\n",
        "        self.switch_player()\n",
        "\n",
        "        # Check for a winner\n",
        "        if all(pile == 0 for pile in self.piles):\n",
        "            self.winner = self.player\n",
        "\n",
        "\n",
        "class NimAI():\n",
        "\n",
        "    def __init__(self, alpha=0.5, epsilon=0.1):\n",
        "        \"\"\"\n",
        "        Initialize AI with an empty Q-learning dictionary,\n",
        "        an alpha (learning) rate, and an epsilon rate.\n",
        "        The Q-learning dictionary maps `(state, action)`\n",
        "        pairs to a Q-value (a number).\n",
        "         - `state` is a tuple of remaining piles, e.g. (1, 1, 4, 4)\n",
        "         - `action` is a tuple `(i, j)` for an action\n",
        "        \"\"\"\n",
        "        self.q = dict()\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def update(self, old_state, action, new_state, reward):\n",
        "        \"\"\"\n",
        "        Update Q-learning model, given an old state, an action taken\n",
        "        in that state, a new resulting state, and the reward received\n",
        "        from taking that action.\n",
        "        \"\"\"\n",
        "        old = self.get_q_value(old_state, action)\n",
        "        best_future = self.best_future_reward(new_state)\n",
        "        self.update_q_value(old_state, action, old, reward, best_future)\n",
        "\n",
        "    def get_q_value(self, state, action):\n",
        "        \"\"\"\n",
        "        Return the Q-value for the state `state` and the action `action`.\n",
        "        If no Q-value exists yet in `self.q`, return 0.\n",
        "        \"\"\"\n",
        "        return self.q[(tuple(state), action)] if (tuple(state), action) in self.q else 0\n",
        "\n",
        "    def update_q_value(self, state, action, old_q, reward, future_rewards):\n",
        "        \"\"\"\n",
        "        Update the Q-value for the state `state` and the action `action`\n",
        "        given the previous Q-value `old_q`, a current reward `reward`,\n",
        "        and an estimate of future rewards `future_rewards`.\n",
        "        Use the formula:\n",
        "        Q(s, a) <- old value estimate\n",
        "                   + alpha * (new value estimate - old value estimate)\n",
        "        where `old value estimate` is the previous Q-value,\n",
        "        `alpha` is the learning rate, and `new value estimate`\n",
        "        is the sum of the current reward and estimated future rewards.\n",
        "        \"\"\"\n",
        "        self.q[(tuple(state), action)] = old_q + self.alpha * (reward + future_rewards - old_q)\n",
        "\n",
        "    def best_future_reward(self, state):\n",
        "        \"\"\"\n",
        "        Given a state `state`, consider all possible `(state, action)`\n",
        "        pairs available in that state and return the maximum of all\n",
        "        of their Q-values.\n",
        "        Use 0 as the Q-value if a `(state, action)` pair has no\n",
        "        Q-value in `self.q`. If there are no available actions in\n",
        "        `state`, return 0.\n",
        "        \"\"\"\n",
        "        best_reward = 0\n",
        "\n",
        "        for action in Nim.available_actions(list(state)):\n",
        "            best_reward = max(self.get_q_value(state, action), best_reward)\n",
        "\n",
        "        return best_reward\n",
        "\n",
        "    def choose_action(self, state, epsilon=True):\n",
        "        \"\"\"\n",
        "        Given a state `state`, return an action `(i, j)` to take.\n",
        "        If `epsilon` is `False`, then return the best action\n",
        "        available in the state (the one with the highest Q-value,\n",
        "        using 0 for pairs that have no Q-values).\n",
        "        If `epsilon` is `True`, then with probability\n",
        "        `self.epsilon` choose a random available action,\n",
        "        otherwise choose the best action available.\n",
        "        If multiple actions have the same Q-value, any of those\n",
        "        options is an acceptable return value.\n",
        "        \"\"\"\n",
        "        best_action = None\n",
        "        best_reward = 0\n",
        "        actions = list(Nim.available_actions(list(state)))\n",
        "\n",
        "        for action in actions:\n",
        "            if best_action is None or self.get_q_value(state, action) > best_reward:\n",
        "                best_reward = self.get_q_value(state, action)\n",
        "                best_action = action\n",
        "\n",
        "        if epsilon:\n",
        "            # Distribute probability weights:\n",
        "            #   (1 - epsilon) --> best_action\n",
        "            #   epsilon       --> among all the other actions\n",
        "            weights = [(1 - self.epsilon) if action == best_action else\n",
        "                       (self.epsilon / (len(actions) - 1)) for action in actions]\n",
        "\n",
        "            best_action = random.choices(actions, weights=weights, k=1)[0]\n",
        "\n",
        "        return best_action\n",
        "\n",
        "\n",
        "def train(n):\n",
        "    \"\"\"\n",
        "    Train an AI by playing `n` games against itself.\n",
        "    \"\"\"\n",
        "\n",
        "    player = NimAI()\n",
        "\n",
        "    # Play n games\n",
        "    for i in range(n):\n",
        "        print(f\"Playing training game {i + 1}\")\n",
        "        game = Nim()\n",
        "\n",
        "        # Keep track of last move made by either player\n",
        "        last = {\n",
        "            0: {\"state\": None, \"action\": None},\n",
        "            1: {\"state\": None, \"action\": None}\n",
        "        }\n",
        "\n",
        "        # Game loop\n",
        "        while True:\n",
        "\n",
        "            # Keep track of current state and action\n",
        "            state = game.piles.copy()\n",
        "            action = player.choose_action(game.piles)\n",
        "\n",
        "            # Keep track of last state and action\n",
        "            last[game.player][\"state\"] = state\n",
        "            last[game.player][\"action\"] = action\n",
        "\n",
        "            # Make move\n",
        "            game.move(action)\n",
        "            new_state = game.piles.copy()\n",
        "\n",
        "            # When game is over, update Q values with rewards\n",
        "            if game.winner is not None:\n",
        "                player.update(state, action, new_state, -1)\n",
        "                player.update(\n",
        "                    last[game.player][\"state\"],\n",
        "                    last[game.player][\"action\"],\n",
        "                    new_state,\n",
        "                    1\n",
        "                )\n",
        "                break\n",
        "\n",
        "            # If game is continuing, no rewards yet\n",
        "            elif last[game.player][\"state\"] is not None:\n",
        "                player.update(\n",
        "                    last[game.player][\"state\"],\n",
        "                    last[game.player][\"action\"],\n",
        "                    new_state,\n",
        "                    0\n",
        "                )\n",
        "\n",
        "    print(\"Done training\")\n",
        "\n",
        "    # Return the trained AI\n",
        "    return player\n",
        "\n",
        "\n",
        "def play(ai, human_player=None):\n",
        "    \"\"\"\n",
        "    Play human game against the AI.\n",
        "    `human_player` can be set to 0 or 1 to specify whether\n",
        "    human player moves first or second.\n",
        "    \"\"\"\n",
        "\n",
        "    # If no player order set, choose human's order randomly\n",
        "    if human_player is None:\n",
        "        human_player = random.randint(0, 1)\n",
        "\n",
        "    # Create new game\n",
        "    game = Nim()\n",
        "\n",
        "    # Game loop\n",
        "    while True:\n",
        "\n",
        "        # Print contents of piles\n",
        "        print()\n",
        "        print(\"Piles:\")\n",
        "        for i, pile in enumerate(game.piles):\n",
        "            print(f\"Pile {i}: {pile}\")\n",
        "        print()\n",
        "\n",
        "        # Compute available actions\n",
        "        available_actions = Nim.available_actions(game.piles)\n",
        "        time.sleep(1)\n",
        "\n",
        "        # Let human make a move\n",
        "        if game.player == human_player:\n",
        "            print(\"Your Turn\")\n",
        "            while True:\n",
        "                pile = int(input(\"Choose Pile: \"))\n",
        "                count = int(input(\"Choose Count: \"))\n",
        "                if (pile, count) in available_actions:\n",
        "                    break\n",
        "                print(\"Invalid move, try again.\")\n",
        "\n",
        "        # Have AI make a move\n",
        "        else:\n",
        "            print(\"AI's Turn\")\n",
        "            pile, count = ai.choose_action(game.piles, epsilon=False)\n",
        "            print(f\"AI chose to take {count} from pile {pile}.\")\n",
        "\n",
        "        # Make move\n",
        "        game.move((pile, count))\n",
        "\n",
        "        # Check for winner\n",
        "        if game.winner is not None:\n",
        "            print()\n",
        "            print(\"GAME OVER\")\n",
        "            winner = \"Human\" if game.winner == human_player else \"AI\"\n",
        "            print(f\"Winner is {winner}\")\n",
        "            return\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}